"""
This module contains functions that take a `FormalContext` (or `MVContext`)
and return a set of formal (or pattern) concepts.
Some of them return a `ConceptLattice` instead of just a set of concepts.

"""
from collections import deque
from typing import List, Tuple, Iterator, Iterable

import numpy as np
import pandas as pd
from bitarray import frozenbitarray as fbarray
from bitarray.util import zeros as bazeros
from caspailleur.order import inverse_order, sort_intents_inclusion

from fcapy.context.formal_context import FormalContext
from fcapy.context.bintable import BinTableBitarray
from fcapy.mvcontext.mvcontext import MVContext
from fcapy.lattice.formal_concept import FormalConcept
from fcapy.lattice.pattern_concept import PatternConcept
from fcapy.utils import utils


def close_by_one(context: MVContext, output_as_concepts=True, iterate_extents=None,
                 initial_combinations=None, iter_concepts_to_check=None):
    """Return a list of concepts generated by CloseByOne (CbO) algorithm

    Parameters
    ----------
    context: `FormalContext` or `MVContext`
        A context to build a set of concepts on
    output_as_concepts: `bool`
        A flag whether to return a list of concepts as a list of `FormalConcept`/`PatternConcept` objects (if set True)
        or as a dictionary with concepts extents and intents
    iterate_extents: `bool`
        A flag whether to run CbO by iterating through subsets of objects (if set True) or of attributes (if set False)
        By default it sets to True if the set of objects is smaller than the set of attributes
    initial_combinations: `list` of `int`
        A list of subsets of objects/attributes indexes (depends on ``iterate_extents``) to start CbO algorithm from
        Default value is empty list []
    iter_concepts_to_check: `list` of `int`
        A list of attributes/objects indexes (depends on ``iterate_extents``) to run CbO algorithm on

    Returns
    -------
    Either ``data`` or ``concepts`` depends on ``output_as_concepts`` attribute
    data: `dict`
        A dictionary which contains a set of concepts extents and concepts intents
    concepts: `list` of `FormalConcept` or `PatternConcept`
        A list of concepts of class `FormalConcept` (if given context is of type `FormalContext`)
        or `PatternConcept` (if given context is of type `MVContext`)

    """
    if iterate_extents is False:
        assert type(context) == FormalContext, "Can set iterate_extents=False only if FormalContext is given"

    if initial_combinations is not None:
        assert iterate_extents is not None,\
            "`iterate_extents parameter should be specified if initial_combinations are given " \
            "(`True if initial_combinations are extents, `False if inital_combinations are intents)"

    if iter_concepts_to_check is not None:
        assert iterate_extents is not None, \
            "`iterate_extents parameter should be specified if iter_concepts_to_check are given " \
            "(`True if iter_concepts_to_check are objects, `False if iter_concepts_to_check are attributes)"

    if iterate_extents is None:
        iterate_extents = context.n_objects < context.n_attributes if type(context) == FormalConcept else True
    n_iters = context.n_objects if iterate_extents else context.n_attributes

    # <iterset> - iterating set - the set of object if one construct construct concepts while iterating over objects,
    #   the set of attributes otherwise
    # <sideset> - the other set, "sided" with <iterset>.
    #   If <iterset> is the set of objects then <sideset> is the set of attributes and vice versa
    iterset_fnc, sideset_fnc = context.extension_i, context.intention_i
    if not iterate_extents:
        iterset_fnc, sideset_fnc = sideset_fnc, iterset_fnc

    iter_concepts_to_check = list(range(n_iters)) if iter_concepts_to_check is None else iter_concepts_to_check

    itersets_i_dict = {}
    sidesets_i = []
    combinations_to_check = [[]] if initial_combinations is None else initial_combinations

    while len(combinations_to_check) > 0:
        comb_i = combinations_to_check.pop(0)
        sideset_i = sideset_fnc(comb_i)
        iterset_i = tuple(iterset_fnc(sideset_i))
        iterset_i_new = sorted(set(iterset_i)-set(comb_i))

        is_not_lexicographic = len(comb_i) > 0 and any([g_i < comb_i[-1] for g_i in iterset_i_new])
        is_duplicate = iterset_i in itersets_i_dict
        if any([is_not_lexicographic, is_duplicate]):
            continue

        itersets_i_dict[iterset_i] = len(sidesets_i)
        sidesets_i.append(sideset_i)

        iterset_i = list(iterset_i)

        new_combs = []
        for g_i in iter_concepts_to_check:
            if g_i not in iterset_i \
                    and (len(comb_i) == 0 or g_i > comb_i[-1]):
                new_combs.append(iterset_i+[g_i])
        combinations_to_check = new_combs + combinations_to_check

    itersets_i = list({idx: x_i for x_i, idx in itersets_i_dict.items()}.values())

    extents_i, intents_i = itersets_i, sidesets_i
    if not iterate_extents:
        extents_i, intents_i = intents_i, extents_i

    if output_as_concepts:
        object_names = context.object_names
        attribute_names = context.attribute_names
        context_hash = context.hash_fixed()

        concepts = []
        for concept_data in zip(extents_i, intents_i):
            extent_i, intent_i = concept_data
            extent = [object_names[g_i] for g_i in extent_i]
            if type(context) == FormalContext:
                intent = [attribute_names[m_i] for m_i in intent_i]
                concept = FormalConcept(extent_i, extent, intent_i, intent, context_hash=context_hash)
            else:
                intent = {context.pattern_structures[ps_i].name: description for ps_i, description in intent_i.items()}
                concept = PatternConcept(
                    extent_i, extent, intent_i, intent,
                    context.pattern_types, context.attribute_names,
                    context_hash=context_hash)
            concepts.append(concept)
        return concepts

    data = {'extents_i': extents_i, 'intents_i': intents_i}
    return data


def close_by_one_objectwise(context: FormalContext or MVContext) -> Iterator[FormalConcept or PatternConcept]:
    """Return a list of concepts generated by CloseByOne (CbO) algorithm

    Parameters
    ----------
    context: `FormalContext` or `MVContext`
        A context to build a set of concepts on

    Returns
    -------
    concepts: `iterator` of `FormalConcept` or `PatternConcept`
        A list of concepts of class `FormalConcept` (if given context is of type `FormalContext`)
        or `PatternConcept` (if given context is of type `MVContext`)

    """
    n_objs = context.n_objects
    object_names, attribute_names = context.object_names, context.attribute_names
    context_hash = context.hash_fixed()

    def create_concept(extent_idxs, intent_idxs):
        extent_idxs = sorted(extent_idxs)
        extent = [object_names[g_i] for g_i in extent_idxs]
        if type(context) == FormalContext:
            intent = [attribute_names[m_i] for m_i in intent_idxs]
            return FormalConcept(extent_idxs, extent, intent_idxs, intent, context_hash=context_hash)

        intent = {context.pattern_structures[ps_i].name: description for ps_i, description in intent_idxs.items()}
        return PatternConcept(extent_idxs, extent, intent_idxs, intent,
                              context.pattern_types, context.attribute_names, context_hash=context_hash)

    extents_i_found = set()
    combinations_to_check = deque([tuple()])

    while combinations_to_check:
        comb_i = combinations_to_check.pop()
        intent_i = context.intention_i(comb_i)
        comb_i_set = set(comb_i)
        if comb_i:
            objects_lexicographic = [g_i for g_i in range(comb_i[-1]) if g_i not in comb_i_set]
            extent_lexicographic = context.extension_i(intent_i, base_objects_i=objects_lexicographic)
            if extent_lexicographic:
                continue

        base_objects_i = range(comb_i[-1]+1, n_objs) if comb_i else range(n_objs)
        base_objects_i = [i for i in base_objects_i if i not in comb_i_set]
        extent_i = comb_i + tuple(context.extension_i(intent_i, base_objects_i=base_objects_i))

        if extent_i in extents_i_found:
            continue

        yield create_concept(extent_i, intent_i)

        possible_new_objects = range(n_objs - 1, (comb_i[-1] if comb_i else 0) - 1, -1)
        extent_i_set = set(extent_i)
        new_combs = [extent_i + (g_i,) for g_i in possible_new_objects if g_i not in extent_i_set]
        combinations_to_check.extend(new_combs)


def close_by_one_objectwise_fbarray(context: FormalContext or MVContext) -> list[FormalConcept or PatternConcept]:
    """Return a list of concepts generated by CloseByOne (CbO) algorithm

    Parameters
    ----------
    context: `FormalContext` or `MVContext`
        A context to build a set of concepts on

    Returns
    -------
    concepts: `iterator` of `FormalConcept` or `PatternConcept`
        A list of concepts of class `FormalConcept` (if given context is of type `FormalContext`)
        or `PatternConcept` (if given context is of type `MVContext`)

    """
    n_objs, n_attrs = context.n_objects, context.n_bin_attrs
    object_names, attribute_names = context.object_names, context.attribute_names
    context_hash = context.hash_fixed()

    def create_concept(extent_idxs: list[int], intent_ba: fbarray):
        extent_idxs = sorted(extent_idxs)
        extent = [object_names[g_i] for g_i in extent_idxs]
        if type(context) == FormalContext:
            intent_idxs = list(intent_ba.itersearch(True))
            intent = [attribute_names[m_i] for m_i in intent_idxs]
            return FormalConcept(extent_idxs, extent, intent_idxs, intent, context_hash=context_hash)

        intent_idxs = context.intention_i(extent_idxs)
        intent = {context.pattern_structures[ps_i].name: description for ps_i, description in intent_idxs.items()}
        return PatternConcept(extent_idxs, extent, intent_idxs, intent,
                              context.pattern_types, context.attribute_names, context_hash=context_hash)

    all_attrs = ~bazeros(n_attrs)

    context_bin = context.binarize() if isinstance(context, MVContext) else context
    objs_descriptions = BinTableBitarray(context_bin.data.data).data

    def intention_ba(objs_idxs: Iterable[int]) -> fbarray:
        intent = all_attrs.copy()
        for g_i in objs_idxs:
            intent &= objs_descriptions[g_i]
        return fbarray(intent)

    def extension_iter(intent_ba: fbarray, base_objects: Iterable[int] = range(n_objs)) -> Iterator[int]:
        for g_i in base_objects:
            if intent_ba & objs_descriptions[g_i] == intent_ba:
                yield g_i

    intents_found: set[fbarray] = set()
    combinations_to_check = deque([tuple()])
    n_cncpt = 0

    while combinations_to_check:
        comb_i = combinations_to_check.pop()
        intent_ba = intention_ba(comb_i)
        if intent_ba in intents_found:
            continue

        comb_i_set = set(comb_i)
        if comb_i:
            objects_lexicographic = (g_i for g_i in range(comb_i[-1]) if g_i not in comb_i_set)
            objects_lexicographic = extension_iter(intent_ba, objects_lexicographic)
            if any(True for _ in objects_lexicographic):
                continue

        base_objects_i = range((comb_i[-1]+1) if comb_i else 0, n_objs)
        base_objects_i = (i for i in base_objects_i if i not in comb_i_set)
        extent_i = comb_i + tuple(extension_iter(intent_ba, base_objects_i))

        yield create_concept(extent_i, intent_ba)
        n_cncpt += 1

        intents_found.add(intent_ba)
        possible_new_objects = range(n_objs - 1, (comb_i[-1] if comb_i else 0) - 1, -1)
        extent_i_set = set(extent_i)
        new_combs = [extent_i + (g_i,) for g_i in possible_new_objects if g_i not in extent_i_set]
        combinations_to_check.extend(new_combs)


def sofia(K: FormalContext or MVContext, L_max: int = 100, use_tqdm: bool = False)\
        -> list[FormalConcept or PatternConcept]:
    from fcapy.lattice import ConceptLattice

    def stability_lbounds(extents: list[fbarray]) -> list[float]:
        children_ordering = inverse_order(sort_intents_inclusion(extents))

        bounds = [
            sum([2 ** (-(extent & ~extents[child_i]).count()) for child_i in children.itersearch(True)])
            for children, extent in zip(children_ordering, extents)
        ]
        return bounds


    extents_proj: list[fbarray] = [fbarray(~bazeros(K.n_objects))]

    n_projs = K.n_bin_attrs
    proj_iterator = utils.safe_tqdm(enumerate(K.to_bin_attr_extents()), total=n_projs,
                               desc='Iter. Sofia projections', disable=not use_tqdm)
    for proj_i, (_, attr_extent_ba) in proj_iterator:
        new_extents = {extent & attr_extent_ba for extent in extents_proj}
        extents_proj = sorted(set(extents_proj) | new_extents, key=lambda extent: extent.count())
        if len(extents_proj) > L_max:
            measure_values = stability_lbounds(extents_proj)
            thold = sorted(measure_values)[::-1][L_max]
            extents_proj = [extent for extent_i, (extent, measure) in enumerate(zip(extents_proj, measure_values))
                            if measure > thold or extent_i in {0, len(extents_proj)-1}]

    def concept_factory(extent_ba: fbarray):
        extent_ids = list(extent_ba.itersearch(True))
        intent_ids = K.intention_i(extent_ids)
        if isinstance(K, FormalContext):
            return FormalConcept(extent_ids, [K.object_names[g_i] for g_i in extent_ids],
                                 intent_ids, [K.attribute_names[m_i] for m_i in intent_ids],
                                 context_hash=K.hash_fixed())

        return PatternConcept(
            extent_ids, [K.object_names[g_i] for g_i in extent_ids],
            intent_ids, {K.pattern_structures[ps_i].name: description
                         for ps_i, description in intent_ids.items()},
            pattern_types=K.pattern_types,
            attribute_names=K.attribute_names,
            context_hash=K.hash_fixed()
        )
    final_concepts = [concept_factory(extent) for extent in extents_proj]
    return final_concepts


def parse_decision_tree_to_extents(tree, X, n_jobs=1) -> List[Tuple[int, ...]]:
    """Return a set of extents of nodes from sklearn `DecisionTree` (or `RandomForest`)

    Parameters
    ----------
    tree: `DecisionTreeRegressor` or `DecisionTreeClassifier` or `RandomForestRegressor` or `RandomForestClassifier`
        sklearn DecisionTree or RandomForest to retrieve a set of extents from
    X: `numpy.ndarray`
        An input data for ``tree`` model. The same format it is used for ``tree.predict(X)`` function
    n_jobs: `int`
        A number of parallel jobs to run. WARNING: A number of jobs works slower than a single one.
    Returns
    -------
    exts: `list` of `int`
        A list of objects indexes from ``X`` described by nodes of decision tree(s) from ``tree``

    """
    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

    if isinstance(tree, (RandomForestClassifier, RandomForestRegressor)):
        paths = tree.decision_path(X)[0].tocsc()
    else:
        paths = tree.decision_path(X).tocsc()

    def get_indices(i, paths):
        return paths.indices[paths.indptr[i]:paths.indptr[i + 1]]
    paths = utils.sparse_unique_columns(paths)[0]

    if n_jobs == 1:
        exts = [get_indices(i, paths) for i in range(paths.shape[1])]
    else:
        from joblib import Parallel, delayed
        exts = Parallel(n_jobs)([delayed(get_indices)(i, paths) for i in range(paths.shape[1])])
    exts = [tuple(ext) for ext in exts]
    return exts


def random_forest_concepts(context: MVContext, rf_params=None, rf_class=None):
    """Fit a RandomForest model and return a set of pattern concepts used by this model

    Parameters
    ----------
    context: `MVContext`
        A context to fit a RandomForest on.
        Training features data for the Forest are kept in context.data
        Target values are kept in ``context.target``
    rf_params: `dict`
        A dict of parameters to initialize RandomForest model with
    rf_class: type `RandomForestRegressor` or `RandomForestClassifier`
        A type of RandomForest model to fit.
        By default this value is set to RandomForestClassifier if ``context.target`` value has only 2 distinct values
    Returns
    -------
    concepts: `list` of `PatternConcept`
        A list of PatternConcepts retrieved from context by RandomForest

    """
    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

    rf_params = rf_params if rf_params is not None else {}

    X = context.to_numeric()[0]
    Y = context.target

    if rf_class is None:
        rf_class = RandomForestClassifier if len(set(Y)) == 2 else RandomForestRegressor

    rf = rf_class(**rf_params)
    rf.fit(X, Y)
    extents_i = parse_decision_tree_to_extents(rf, X)
    extents_i.append(context.extension_i(context.intention_i([])))

    concepts = []

    object_names = context.object_names
    context_hash = context.hash_fixed()
    for extent_i in extents_i:
        extent = [object_names[g_i] for g_i in extent_i]
        intent_i = context.intention_i(extent_i)
        if type(context) == FormalContext:
            intent = [context.attribute_names[m_i] for m_i in intent_i]
            concept = FormalConcept(extent_i, extent, intent_i, intent, context_hash=context_hash)
        else:
            intent = {context.pattern_structures[ps_i].name: description for ps_i, description in intent_i.items()}
            concept = PatternConcept(extent_i, extent, intent_i, intent,
                                     context.pattern_types, context.attribute_names, context_hash=context_hash)
        concepts.append(concept)

    return concepts


def lindig_algorithm(context: FormalContext, iterate_extents=None):
    """Get Concept Lattice from Formal Context using Lindig algorithm
    (https://www.researchgate.net/publication/2812391_Fast_Concept_Analysis)

    Parameters
    ----------
    context: `FormalContext`
        A context to build lattice on
    iterate_extents: `bool`
        A flag whether to run Lindig by iterating through subsets of objects (if set True) or of attributes (if set False)
        By default it sets to True if the set of objects is smaller than the set of attributes and False otherwise

    Returns
    -------
    lattice: `ConceptLattice`
        A ConceptLattice which contains a set of Formal Concepts and relations between them

    """
    if type(context) == MVContext:
        raise NotImplementedError('Sorry. Lindig algorithm is not yet implemented for ManyValued context')

    from fcapy.lattice import ConceptLattice

    if iterate_extents is None:
        iterate_extents = context.n_objects < context.n_attributes

    n_objects = context.n_objects
    n_attributes = context.n_attributes
    intention_i = context.intention_i
    extension_i = context.extension_i
    object_names = context.object_names
    attribute_names = context.attribute_names
    if not iterate_extents:
        n_objects, n_attributes = n_attributes, n_objects
        intention_i, extension_i = extension_i, intention_i
        object_names, attribute_names = attribute_names, object_names
    context_hash = context.hash_fixed()


    def direct_super_concepts(concept):
        extent = set(concept.extent_i)
        reps = set(range(n_objects)) - extent
        neighbors = []
        for g in set(reps):
            extent.add(g)
            M = intention_i(list(extent))
            G = extension_i(M)
            extent.remove(g)
            if len(reps & set(G)) == 1:
                neighbors.append(FormalConcept(G, [object_names[i] for i in G],
                                               M, [attribute_names[i] for i in M],
                                               context_hash = context_hash))
            else:
                reps.remove(g)
        return neighbors

    M = list(range(n_attributes))
    G = extension_i(M)
    c = FormalConcept(G, [object_names[i] for i in G],
                      M, [attribute_names[i] for i in M],
                      context_hash = context_hash)

    concepts = [c]
    queue = {c}
    children_dict = {0: []}
    parents_dict = {}

    index = {c : 0}

    while len(queue) != 0:
        c = queue.pop()
        c_id = index[c]
        dsups = direct_super_concepts(c)
        if len(dsups) == 0:
            parents_dict[c_id] = []
            continue

        for x in dsups:
            if x not in index:
                queue.add(x)
                index[x] = len(concepts)
                concepts.append(x)
            x_id = index[x]

            children_dict.setdefault(x_id, []).append(c_id)
            parents_dict.setdefault(c_id, []).append(x_id)

    if not iterate_extents:
        concepts = [FormalConcept(concepts[i].intent_i, concepts[i].intent,
                                  concepts[i].extent_i, concepts[i].extent, 
                                  context_hash=context_hash)
                    for i in range(len(concepts))]
        children_dict, parents_dict = parents_dict, children_dict

    lattice = ConceptLattice(concepts, children_dict=children_dict)
    return lattice


from datetime import datetime

if __name__ == '__main__':
    K = FormalContext.read_csv('../../data/mango_bin.csv')

    times1 = []
    for _ in range(1000):
        t1 = datetime.now()
        close_by_one(K, output_as_concepts=True, iterate_extents=True)
        t2 = datetime.now()
        times1.append((t2-t1).total_seconds())
    print('orig', np.mean(times1))

    times2 = []
    for _ in range(1000):
        t1 = datetime.now()
        list(close_by_one_objectwise(K))
        t2 = datetime.now()
        times2.append((t2 - t1).total_seconds())
    print('new', np.mean(times2))

    times3 = []
    for _ in range(1000):
        t1 = datetime.now()
        list(close_by_one_objectwise_fbarray(K))
        t2 = datetime.now()
        times3.append((t2 - t1).total_seconds())
    print('new_ba', np.mean(times3))
    print(np.mean(times3)/np.mean(times1), np.mean(times2)/np.mean(times1))

    assert set(close_by_one(K, output_as_concepts=True)) == set(close_by_one_objectwise(K))
    print(len(list(close_by_one_objectwise_fbarray(K))), len(list(close_by_one(K, output_as_concepts=True))))

    cbo_orig = list(close_by_one(K, output_as_concepts=True))
    cbo_new = list(close_by_one_objectwise_fbarray(K))
    print(set(cbo_orig) == set(cbo_new))

    #assert set(cbo_orig) == set(cbo_new)
    #assert set(close_by_one(K, output_as_concepts=True, iterate_extents=True)) == set(close_by_one_objectwise(K))
    #assert close_by_one(K, output_as_concepts=True) == list(close_by_one_objectwise(K))
    print("XXX")

    import fcapy.mvcontext.pattern_structure as PS
    df = pd.read_csv('../../data/mango.csv', index_col=0)
    ptypes = {f: PS.SetPS for f in df.columns}
    mvK = MVContext(df.values, ptypes, object_names=list(df.index), attribute_names=list(df.columns))
    mvcbo_orig = close_by_one(mvK)
    mvcbo_new = list(close_by_one_objectwise(mvK))
    print(len(mvcbo_orig), len(mvcbo_orig))
    #assert mvcbo_orig == mvcbo_new

    times1 = []
    for _ in range(1000):
        t1 = datetime.now()
        close_by_one(mvK, output_as_concepts=True, iterate_extents=True)
        t2 = datetime.now()
        times1.append((t2 - t1).total_seconds())
    print('orig', np.mean(times1))

    times2 = []
    for _ in range(1000):
        t1 = datetime.now()
        list(close_by_one_objectwise(mvK))
        t2 = datetime.now()
        times2.append((t2 - t1).total_seconds())
    print('new', np.mean(times2))

    times3 = []
    for _ in range(1000):
        t1 = datetime.now()
        list(close_by_one_objectwise_fbarray(mvK))
        t2 = datetime.now()
        times3.append((t2 - t1).total_seconds())
    print('new_ba', np.mean(times3))
    print(1 - np.mean(times3) / np.mean(times1), 1 - np.mean(times2) / np.mean(times1))
